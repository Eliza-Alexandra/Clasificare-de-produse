path = "/kaggle/input/products-images/images"
import os

from tensorflow.keras.models import Sequential
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input
from tensorflow.keras.optimizers import Adam

import tensorflow as tf

train_dataset = tf.keras.preprocessing.image_dataset_from_directory(
    path,
    validation_split=0.3,  # 70% antrenare, 30% validare+testare
    subset="training",
    seed=42, # val fixa pt generarea aleatoare
    image_size=(224, 224), 
    batch_size=64
)

val_test_dataset = tf.keras.preprocessing.image_dataset_from_directory(
    path,
    validation_split=0.3,  # 30% val+test
    subset="validation",
    seed=42,
    image_size=(224, 224),
    batch_size=64
)

total_batches = tf.data.experimental.cardinality(val_test_dataset).numpy() #nr batch-uri in integer py

val_batches = int(total_batches * 2 / 3)
test_batches = total_batches - val_batches

val_dataset = val_test_dataset.take(val_batches)
test_dataset = val_test_dataset.skip(val_batches)

def resize_with_padding(image, label):
    image = tf.image.convert_image_dtype(image, tf.float32)
    image = tf.image.resize_with_pad(image, 224, 224)
    return image, label

def augment(image, label):
    image = tf.image.random_brightness(image, 0.2)
    image = tf.image.random_contrast(image, 0.8, 1.2)
    image = tf.image.random_flip_left_right(image)
    image = tf.image.random_flip_up_down(image)
    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)
    image = tf.image.random_hue(image, max_delta=0.02)
    return image, label

#antrenare: resize + augmentare + preprocess_input
train_dataset = train_dataset.map(resize_with_padding, num_parallel_calls=tf.data.AUTOTUNE)
train_dataset = train_dataset.map(augment, num_parallel_calls=tf.data.AUTOTUNE)
train_dataset = train_dataset.map(lambda x, y: (preprocess_input(x), y), num_parallel_calls=tf.data.AUTOTUNE)
train_dataset = train_dataset.shuffle(1000).prefetch(tf.data.AUTOTUNE)

#validare, test: doar resize + preprocess_input
val_dataset = val_dataset.map(resize_with_padding, num_parallel_calls=tf.data.AUTOTUNE)
val_dataset = val_dataset.map(lambda x, y: (preprocess_input(x), y), num_parallel_calls=tf.data.AUTOTUNE)
val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)

test_dataset = test_dataset.map(resize_with_padding, num_parallel_calls=tf.data.AUTOTUNE)
test_dataset = test_dataset.map(lambda x, y: (preprocess_input(x), y), num_parallel_calls=tf.data.AUTOTUNE)
test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)

base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

#for layer in base_model.layers:
    #layer.trainable = False

for layer in base_model.layers[-30:]: 
    layer.trainable = True

from tensorflow.keras.layers import Dropout, Dense, GlobalAveragePooling2D, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.regularizers import l2

v16_model = Sequential([ #construire model strat cu strat
    base_model, #model vgg16 preantrenat
    GlobalAveragePooling2D(), #multidim ->unidim (pregatire Dense)
    Dropout(0.5),  # 50% dropout
    Dense(256, activation='relu', kernel_regularizer=l2(1e-3)), #256 neuroni, fct activare
    BatchNormalization(),
    Dropout(0.2),
    Dense(10, activation='softmax'), #strat iesire, nr neuroni=nr clase
])

v16_model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

checkpoint = ModelCheckpoint(
    "model_checkpoint.keras",
    monitor="val_loss",
    save_best_only=True,
    verbose=1
)

if os.path.exists("model_checkpoint.keras"):
    print("Se incarca modelul salvat...")
    v16_model = tf.keras.models.load_model("model_checkpoint.keras")
else:
    print("Nu exista model salvat")

history = v16_model.fit(
    train_dataset,
    epochs=60,
    validation_data=val_dataset,
    callbacks=[checkpoint]
)

loss, accuracy = v16_model.evaluate(test_dataset)



